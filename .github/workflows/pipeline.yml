name: Full Pipeline (Infra + App)

on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      action:
        description: "Choose deploy or destroy"
        required: true
        default: deploy
        type: choice
        options:
          - deploy
          - destroy
      environment:
        description: "Environment folder under terraform/envs (e.g., dev)"
        required: true
        default: dev

permissions:
  contents: read

env:
  AWS_DEFAULT_REGION: us-west-2
  AWS_REGION: us-west-2
  CLUSTER_NAME: restaurant-app

jobs:
  bootstrap:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    outputs:
      bucket: ${{ steps.outputs_step.outputs.bucket }}
    steps:
      - uses: actions/checkout@v4

      - uses: hashicorp/setup-terraform@v3

      - name: Ensure S3 backend bucket exists
        id: ensure_bucket
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET="restaurant-app-tfstate-${ACCOUNT_ID}"
          if aws s3api head-bucket --bucket "$BUCKET" >/dev/null 2>&1; then
            echo "S3 state bucket already exists: $BUCKET"
            echo "bucket=$BUCKET" >> $GITHUB_OUTPUT
          else
            echo "Creating S3 state bucket via Terraform bootstrap"
            terraform -chdir=terraform/bootstrap init -input=false
            terraform -chdir=terraform/bootstrap apply -auto-approve -input=false \
              -var aws_region=$AWS_REGION -var project_name=restaurant-app
            # Read output (should match computed name)
            TF_BUCKET=$(terraform -chdir=terraform/bootstrap output -raw backend_bucket)
            echo "bucket=${TF_BUCKET}" >> $GITHUB_OUTPUT
          fi

      - name: Pass bucket output
        id: outputs_step
        run: |
          echo "bucket=${{ steps.ensure_bucket.outputs.bucket }}" >> $GITHUB_OUTPUT

  infra:
    needs: bootstrap
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    outputs:
      applied: ${{ steps.set_applied.outputs.applied }}
      cluster_name: ${{ steps.export_cluster.outputs.cluster_name }}
      alb_irsa: ${{ steps.export_cluster.outputs.alb_irsa }}
    steps:
      - uses: actions/checkout@v4

      - uses: hashicorp/setup-terraform@v3

      - name: Terraform Init with remote backend
        working-directory: terraform/envs/${{ github.event.inputs.environment || 'dev' }}
        run: |
          terraform init -input=false \
            -backend-config="bucket=${{ needs.bootstrap.outputs.bucket }}" \
            -backend-config="key=envs/${{ github.event.inputs.environment || 'dev' }}/terraform.tfstate" \
            -backend-config="region=$AWS_REGION" \
            -backend-config="encrypt=true"

      - name: Import pre-existing ECR repositories (if any)
        working-directory: terraform/envs/${{ github.event.inputs.environment || 'dev' }}
        run: |
          set -e
          for repo in restaurant-app-backend restaurant-app-frontend; do
            if aws ecr describe-repositories --repository-names "$repo" >/dev/null 2>&1; then
              # Import only if not already in state
              if ! terraform state show aws_ecr_repository.${repo##restaurant-app-} >/dev/null 2>&1; then
                # Map repo name to TF resource name (backend/frontend)
                rname=$(echo "$repo" | awk -F'-' '{print $3}')
                terraform import aws_ecr_repository.$rname "$repo" || true
              fi
            fi
          done

      - name: Import existing EKS KMS key and alias (if any)
        working-directory: terraform/envs/${{ github.event.inputs.environment || 'dev' }}
        run: |
          set -e
          ALIAS="alias/eks/${CLUSTER_NAME}"
          if aws kms describe-key --key-id "$ALIAS" >/dev/null 2>&1; then
            KEY_ARN=$(aws kms describe-key --key-id "$ALIAS" --query 'KeyMetadata.Arn' --output text)
            terraform import 'module.eks.module.kms.aws_kms_alias.this["cluster"]' "$ALIAS" || true
            terraform import 'module.eks.module.kms.aws_kms_key.this["cluster"]' "$KEY_ARN" || true
          fi

      - name: Terraform Apply
        if: ${{ github.event.inputs.action == 'deploy' || github.event_name == 'push' }}
        working-directory: terraform/envs/${{ github.event.inputs.environment || 'dev' }}
        run: |
          terraform apply -auto-approve -input=false \
            -var aws_region=$AWS_REGION -var project_name=restaurant-app

      - name: Wait for EKS cluster to be active
        if: ${{ github.event.inputs.action == 'deploy' || github.event_name == 'push' }}
        run: |
          for i in {1..30}; do
            if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" --query 'cluster.status' --output text | grep -q Active; then
              echo "Cluster is Active"; break; fi; echo "Waiting for cluster..."; sleep 20; done

      - name: Wait for at least 1 Ready node
        if: ${{ github.event.inputs.action == 'deploy' || github.event_name == 'push' }}
        run: |
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$AWS_REGION"
          for i in {1..30}; do
            READY=$(kubectl get nodes --no-headers 2>/dev/null | awk '{print $2}' | grep -c Ready || true)
            if [ "$READY" -ge 1 ]; then echo "Nodes ready: $READY"; break; fi; echo "Waiting for nodes..."; sleep 20; done
          kubectl get nodes -o wide || true
          aws eks describe-nodegroup --cluster-name "$CLUSTER_NAME" --nodegroup-name default --region "$AWS_REGION" || true

      - name: Export cluster name
        if: ${{ github.event.inputs.action == 'deploy' || github.event_name == 'push' }}
        id: export_cluster
        working-directory: terraform/envs/${{ github.event.inputs.environment || 'dev' }}
        run: |
          echo "cluster_name=$(terraform output -raw cluster_name)" >> $GITHUB_OUTPUT
          echo "alb_irsa=$(terraform output -raw aws_lb_controller_role_arn)" >> $GITHUB_OUTPUT

      - name: "Pre-destroy: cleanup Kubernetes resources"
        if: ${{ github.event.inputs.action == 'destroy' }}
        run: |
          if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" >/dev/null 2>&1; then
            curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v1.29.0/bin/linux/amd64/kubectl
            sudo install kubectl /usr/local/bin/kubectl
            curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
            aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$AWS_REGION"
            helm -n kube-system uninstall aws-load-balancer-controller || true
            kubectl delete namespace restaurant --ignore-not-found --wait=true || true
          else
            echo "Cluster not found; skipping k8s cleanup."
          fi

      - name: Terraform Destroy
        if: ${{ github.event.inputs.action == 'destroy' }}
        working-directory: terraform/envs/${{ github.event.inputs.environment || 'dev' }}
        run: |
          terraform destroy -auto-approve -input=false \
            -var aws_region=$AWS_REGION -var project_name=restaurant-app

      - name: Mark applied
        id: set_applied
        run: echo "applied=true" >> $GITHUB_OUTPUT

  app:
    needs: infra
    if: ${{ needs.infra.outputs.applied == 'true' && (github.event.inputs.action == 'deploy' || github.event_name == 'push') }}
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      ECR_BACKEND: restaurant-app-backend
      ECR_FRONTEND: restaurant-app-frontend
    steps:
      - uses: actions/checkout@v4

      - name: Use cluster name from Terraform output
        if: ${{ needs.infra.outputs.cluster_name != '' }}
        run: echo "CLUSTER_NAME=${{ needs.infra.outputs.cluster_name }}" >> $GITHUB_ENV

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push backend image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_REPO: ${{ env.ECR_BACKEND }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker build -f Dockerfile.backend -t $ECR_REGISTRY/$IMAGE_REPO:$IMAGE_TAG .
          docker push $ECR_REGISTRY/$IMAGE_REPO:$IMAGE_TAG

      - name: Build and push frontend image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_REPO: ${{ env.ECR_FRONTEND }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker build -f menu-frontend/Dockerfile -t $ECR_REGISTRY/$IMAGE_REPO:$IMAGE_TAG menu-frontend
          docker push $ECR_REGISTRY/$IMAGE_REPO:$IMAGE_TAG

      - name: Check EKS cluster exists
        id: check-eks
        run: |
          if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" >/dev/null 2>&1; then
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Install kubectl
        if: steps.check-eks.outputs.exists == 'true'
        run: |
          curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v1.29.0/bin/linux/amd64/kubectl
          sudo install kubectl /usr/local/bin/kubectl

      - name: Update kubeconfig
        if: steps.check-eks.outputs.exists == 'true'
        run: |
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$AWS_REGION"
          # Test auth
          kubectl auth can-i list pods --all-namespaces || true

      - name: Apply Kubernetes base manifests (no Ingress yet)
        if: steps.check-eks.outputs.exists == 'true'
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          kubectl apply --validate=false -f terraform/k8s/namespace.yaml
          kubectl apply --validate=false -f terraform/k8s/storageclass-standard.yaml
          kubectl apply --validate=false -f terraform/k8s/configmap.yaml
          # Apply deployments and wait for rollout (backend now uses emptyDir)
          sed -e "s|<ECR_BACKEND_IMAGE>|$ECR_REGISTRY/${{ env.ECR_BACKEND }}|g" -e "s|<TAG>|$IMAGE_TAG|g" terraform/k8s/backend-deployment.yaml | kubectl apply --validate=false -f -
          sed -e "s|<ECR_FRONTEND_IMAGE>|$ECR_REGISTRY/${{ env.ECR_FRONTEND }}|g" -e "s|<TAG>|$IMAGE_TAG|g" terraform/k8s/frontend-deployment.yaml | kubectl apply --validate=false -f -
          kubectl -n restaurant rollout status deploy/backend --timeout=180s || true
          # Create PVC and wait for binding
          kubectl apply --validate=false -f terraform/k8s/sqlite-pvc.yaml
          for i in {1..30}; do
            phase=$(kubectl -n restaurant get pvc sqlite-pvc -o jsonpath='{.status.phase}' 2>/dev/null || echo "");
            if [ "$phase" = "Bound" ]; then echo "PVC bound"; break; fi;
            echo "Waiting for PVC to bind..."; sleep 10;
          done
          # Restart backend to pick up PVC
          kubectl -n restaurant rollout restart deploy/backend || true
          kubectl -n restaurant rollout status deploy/backend --timeout=180s || true


      - name: Apply IngressClass for AWS ALB controller
        if: steps.check-eks.outputs.exists == 'true'
        run: |
          kubectl apply --validate=false -f terraform/k8s/ingressclass.yaml

      # Let Helm install CRDs for the AWS Load Balancer Controller

      - name: "Debug: list namespaces and resources"
        if: steps.check-eks.outputs.exists == 'true'
        run: |
          kubectl get ns
          kubectl -n restaurant get all,ingress,pvc || true

      - name: Install AWS Load Balancer Controller
        if: steps.check-eks.outputs.exists == 'true'
        run: |
          # Install or upgrade the controller via Helm
          helm repo add eks https://aws.github.io/eks-charts || true
          helm repo update
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName="$CLUSTER_NAME" \
            --set region="$AWS_REGION" \
            --set serviceAccount.create=true \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"='${{ needs.infra.outputs.alb_irsa }}' \
            --set vpcId=$(aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" --query 'cluster.resourcesVpcConfig.vpcId' --output text) \
            --set installCRDs=true

      - name: Wait for ALB controller to be ready
        if: steps.check-eks.outputs.exists == 'true'
        run: |
          kubectl -n kube-system rollout status deploy/aws-load-balancer-controller --timeout=180s || true

      - name: Verify CRDs and IRSA binding
        if: steps.check-eks.outputs.exists == 'true'
        run: |
          echo "Checking CRDs..." | tee -a $GITHUB_STEP_SUMMARY
          kubectl api-resources | grep -E "elbv2.k8s.aws|targetgroupbindings" || true
          echo "ServiceAccount annotations:" | tee -a $GITHUB_STEP_SUMMARY
          kubectl -n kube-system get sa aws-load-balancer-controller -o jsonpath='{.metadata.annotations}' | tee -a $GITHUB_STEP_SUMMARY || true
          echo

      - name: Apply Ingress
        if: steps.check-eks.outputs.exists == 'true'
        run: |
          kubectl apply --validate=false -f terraform/k8s/ingress.yaml

      - name: Wait for ALB Ingress and expose URL
        if: steps.check-eks.outputs.exists == 'true'
        run: |
          for i in {1..60}; do
            HOST=$(kubectl -n restaurant get ingress restaurant-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
            if [ -n "$HOST" ]; then echo "Application URL: http://$HOST" | tee -a $GITHUB_STEP_SUMMARY; break; fi
            echo "Waiting for Ingress hostname..."; kubectl -n restaurant get ingress restaurant-ingress -o wide || true; sleep 10;
          done

      # Removed step to set frontend API base; frontend uses relative API now

      - name: "Debug: ALB controller status"
        if: steps.check-eks.outputs.exists == 'true'
        run: |
          kubectl -n kube-system get deploy aws-load-balancer-controller -o wide || true
          kubectl -n kube-system logs deploy/aws-load-balancer-controller --tail=50 || true

      - name: "Summarize endpoints"
        if: steps.check-eks.outputs.exists == 'true'
        run: |
          HOST=$(kubectl -n restaurant get ingress restaurant-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
          if [ -n "$HOST" ]; then
            echo "Application URL: http://$HOST" | tee -a $GITHUB_STEP_SUMMARY
            echo "Health: http://$HOST/health" | tee -a $GITHUB_STEP_SUMMARY
          else
            echo "Ingress hostname not available yet." | tee -a $GITHUB_STEP_SUMMARY
          fi

      - name: "Debug: Ingress and TGB"
        if: steps.check-eks.outputs.exists == 'true'
        run: |
          kubectl -n restaurant describe ingress restaurant-ingress || true
          kubectl get targetgroupbindings.elbv2.k8s.aws -A || true

